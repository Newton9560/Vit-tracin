{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from torchvision import transforms\n",
    "import my_dataset\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from models.vit import ViT\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 6\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1.98e-4,1.925e-4,1.8e-4,1.67e-4,1.5e-4,1.3e-4]\n",
    "lrs2 = [3e-4,2.5e-4,2.3e-4,2e-4,1.8e-4,1.5e-4]\n",
    "writer = SummaryWriter(log_dir = 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 dataloader workers every process\n"
     ]
    }
   ],
   "source": [
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "print('Using {} dataloader workers every process'.format(nw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.read_file(\"./tracin_file/checkpoint\" + str(checkpoint) + \"0_0.3.txt\")\n",
    "val_data = utils.read_file(\"../cifar10/val_data.txt\")\n",
    "data_transform = {\n",
    "        \"train\": transforms.Compose([\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.Resize(32),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "        \"val\": transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])}\n",
    "train_dataset = my_dataset.MyDataSet_CIFAR(images_path=train_data,\n",
    "                        transform=data_transform[\"train\"])\n",
    "\n",
    "val_dataset = my_dataset.MyDataSet_CIFAR(images_path=val_data,\n",
    "                        transform=data_transform[\"val\"])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=nw,\n",
    "                                            collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
       "    (1): Linear(in_features=48, out_features=512, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_latent): Identity()\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./weights/model-\"+ str(checkpoint) + \"0.pth\", map_location=device))\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lrs2[checkpoint-1])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100 - checkpoint * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train epoch 0] loss: 0.474, acc: 0.830: 100%|██████████| 329/329 [00:16<00:00, 20.07it/s]\n",
      "[valid epoch 0] loss: 1.454, acc: 0.606: 100%|██████████| 313/313 [00:04<00:00, 63.48it/s]\n",
      "[train epoch 1] loss: 0.438, acc: 0.848: 100%|██████████| 329/329 [00:15<00:00, 20.60it/s]\n",
      "[valid epoch 1] loss: 1.709, acc: 0.573: 100%|██████████| 313/313 [00:04<00:00, 62.84it/s]\n",
      "[train epoch 2] loss: 0.395, acc: 0.860: 100%|██████████| 329/329 [00:16<00:00, 20.56it/s]\n",
      "[valid epoch 2] loss: 1.663, acc: 0.583: 100%|██████████| 313/313 [00:04<00:00, 62.78it/s]\n",
      "[train epoch 3] loss: 0.369, acc: 0.871: 100%|██████████| 329/329 [00:15<00:00, 20.58it/s]\n",
      "[valid epoch 3] loss: 1.752, acc: 0.572: 100%|██████████| 313/313 [00:04<00:00, 62.62it/s]\n",
      "[train epoch 4] loss: 0.334, acc: 0.881: 100%|██████████| 329/329 [00:16<00:00, 20.54it/s]\n",
      "[valid epoch 4] loss: 1.757, acc: 0.577: 100%|██████████| 313/313 [00:05<00:00, 62.46it/s]\n",
      "[train epoch 5] loss: 0.329, acc: 0.884: 100%|██████████| 329/329 [00:16<00:00, 20.46it/s]\n",
      "[valid epoch 5] loss: 2.013, acc: 0.552: 100%|██████████| 313/313 [00:05<00:00, 62.54it/s]\n",
      "[train epoch 6] loss: 0.309, acc: 0.891: 100%|██████████| 329/329 [00:16<00:00, 20.50it/s]\n",
      "[valid epoch 6] loss: 2.001, acc: 0.555: 100%|██████████| 313/313 [00:05<00:00, 62.41it/s]\n",
      "[train epoch 7] loss: 0.288, acc: 0.897: 100%|██████████| 329/329 [00:16<00:00, 20.48it/s]\n",
      "[valid epoch 7] loss: 1.969, acc: 0.569: 100%|██████████| 313/313 [00:05<00:00, 62.43it/s]\n",
      "[train epoch 8] loss: 0.264, acc: 0.904: 100%|██████████| 329/329 [00:16<00:00, 20.36it/s]\n",
      "[valid epoch 8] loss: 2.101, acc: 0.556: 100%|██████████| 313/313 [00:05<00:00, 62.40it/s]\n",
      "[train epoch 9] loss: 0.259, acc: 0.906: 100%|██████████| 329/329 [00:16<00:00, 20.35it/s]\n",
      "[valid epoch 9] loss: 2.149, acc: 0.560: 100%|██████████| 313/313 [00:05<00:00, 62.13it/s]\n",
      "[train epoch 10] loss: 0.239, acc: 0.914: 100%|██████████| 329/329 [00:16<00:00, 20.32it/s]\n",
      "[valid epoch 10] loss: 2.228, acc: 0.561: 100%|██████████| 313/313 [00:05<00:00, 62.40it/s]\n",
      "[train epoch 11] loss: 0.224, acc: 0.918: 100%|██████████| 329/329 [00:16<00:00, 20.29it/s]\n",
      "[valid epoch 11] loss: 2.166, acc: 0.563: 100%|██████████| 313/313 [00:05<00:00, 62.19it/s]\n",
      "[train epoch 12] loss: 0.215, acc: 0.924: 100%|██████████| 329/329 [00:16<00:00, 20.30it/s]\n",
      "[valid epoch 12] loss: 2.404, acc: 0.547: 100%|██████████| 313/313 [00:05<00:00, 62.11it/s]\n",
      "[train epoch 13] loss: 0.193, acc: 0.931: 100%|██████████| 329/329 [00:16<00:00, 20.28it/s]\n",
      "[valid epoch 13] loss: 2.500, acc: 0.543: 100%|██████████| 313/313 [00:05<00:00, 61.88it/s]\n",
      "[train epoch 14] loss: 0.185, acc: 0.936: 100%|██████████| 329/329 [00:16<00:00, 20.24it/s]\n",
      "[valid epoch 14] loss: 2.489, acc: 0.549: 100%|██████████| 313/313 [00:05<00:00, 61.88it/s]\n",
      "[train epoch 15] loss: 0.174, acc: 0.938: 100%|██████████| 329/329 [00:16<00:00, 20.19it/s]\n",
      "[valid epoch 15] loss: 2.467, acc: 0.559: 100%|██████████| 313/313 [00:05<00:00, 61.81it/s]\n",
      "[train epoch 16] loss: 0.164, acc: 0.941: 100%|██████████| 329/329 [00:16<00:00, 20.21it/s]\n",
      "[valid epoch 16] loss: 2.390, acc: 0.570: 100%|██████████| 313/313 [00:05<00:00, 62.00it/s]\n",
      "[train epoch 17] loss: 0.144, acc: 0.950: 100%|██████████| 329/329 [00:16<00:00, 20.18it/s]\n",
      "[valid epoch 17] loss: 2.697, acc: 0.551: 100%|██████████| 313/313 [00:05<00:00, 62.03it/s]\n",
      "[train epoch 18] loss: 0.143, acc: 0.948: 100%|██████████| 329/329 [00:16<00:00, 20.22it/s]\n",
      "[valid epoch 18] loss: 2.702, acc: 0.548: 100%|██████████| 313/313 [00:05<00:00, 61.75it/s]\n",
      "[train epoch 19] loss: 0.125, acc: 0.957: 100%|██████████| 329/329 [00:16<00:00, 20.19it/s]\n",
      "[valid epoch 19] loss: 2.850, acc: 0.547: 100%|██████████| 313/313 [00:05<00:00, 61.86it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(100 - 10 * checkpoint):\n",
    "    \n",
    "    model.train()\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "    accu_num = torch.zeros(1).to(device)  # 累计预测正确的样本数\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(train_loader)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        \n",
    "        pred_classes = torch.max(pred, dim=1)[1]  # 预测的类别，[1]是标签索引\n",
    "       \n",
    "        \n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        accu_loss += loss.detach()\n",
    "        \n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "        optimizer.step()  # 更新\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_loss =  accu_loss.item() / (step + 1)\n",
    "    train_acc = accu_num.item() / sample_num\n",
    "    val_loss, val_acc = utils.evaluate(model=model,\n",
    "                                data_loader=val_loader,\n",
    "                                device=device,\n",
    "                                epoch=epoch)\n",
    "    writer.add_scalar(tags[0] + \"_\" + str(checkpoint*10), train_loss, epoch + 10 * checkpoint)\n",
    "    writer.add_scalar(tags[1] + \"_\" + str(checkpoint*10), train_acc, epoch + 10 * checkpoint)\n",
    "    writer.add_scalar(tags[2] + \"_\" + str(checkpoint*10), val_loss, epoch + 10 * checkpoint)\n",
    "    writer.add_scalar(tags[3] + \"_\" + str(checkpoint*10), val_acc, epoch + 10 * checkpoint)\n",
    "    writer.add_scalar(tags[4] + \"_\" + str(checkpoint*10), optimizer.param_groups[0][\"lr\"], epoch + 10 * checkpoint)\n",
    "    scheduler.step()\n",
    "    i = i + 1\n",
    "    if i == 20:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2c9fc71220d1b58b3640599b1e22027da1326aa67720425adf09ad6c638495"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
